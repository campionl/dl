# üî• Guida alle Funzioni di Attivazione nelle Reti Neurali

## ‚öôÔ∏è Cos'√® una funzione di attivazione?
Una funzione di attivazione √® ci√≤ che permette a un neurone artificiale di "decidere" se e quanto attivarsi.  
Serve a introdurre **non-linearit√†**, cos√¨ che la rete possa risolvere problemi complessi (come classificazioni, immagini, ecc.).

---

## üìò Funzioni di attivazione principali

### 1. ReLU (Rectified Linear Unit)
`f(x) = max(0, x)`

- Lascia passare solo i numeri positivi
- √à molto veloce da calcolare
- Usata nei layer **nascosti**
- Problema: pu√≤ "bloccare" neuroni se ricevono valori negativi per troppo tempo (problema dei neuroni morti)

---

### 2. Leaky ReLU
`f(x) = x se x > 0, altrimenti Œ±x (tipicamente Œ± = 0.01)`

- Variante della ReLU
- Permette un piccolo flusso anche quando x √® negativa
- Previene i neuroni morti
- Ideale quando noti che la tua rete "smette di imparare"

---

### 3. Sigmoid
`f(x) = 1 / (1 + e^(-x))`

- Trasforma il valore in un numero tra 0 e 1
- Buona per classificazione **binaria**
- Interpretabile come **probabilit√†**
- Problema: non √® centrata sullo 0 ‚Üí pu√≤ rallentare l‚Äôapprendimento (vanishing gradient)

---

### 4. Tanh (Tangente iperbolica)
`f(x) = (e^x - e^-x) / (e^x + e^-x)`

- Restituisce valori tra **-1 e 1**
- Simile alla sigmoid, ma **centrata**
- Migliore nei layer nascosti rispetto a sigmoid
- Anche lei pu√≤ avere problemi di vanishing gradient

---

### 5. Softmax
`f(x·µ¢) = e^(x·µ¢) / Œ£ e^(x‚±º)`

- Usata solo nel **layer di output**
- Converte un vettore di valori in **probabilit√†**
- Ideale per classificazione **multiclasse** (3 o pi√π classi)
- La somma di tutti i valori in output √® sempre 1

---

## üìä Tabella Riepilogativa

| Funzione     | Output       | Quando si usa                              |
|--------------|--------------|--------------------------------------------|
| ReLU         | [0, ‚àû)       | Nei layer nascosti                         |
| Leaky ReLU   | (-‚àû, ‚àû)      | Layer nascosti, se ReLU blocca i neuroni   |
| Sigmoid      | (0, 1)       | Output per classificazione binaria         |
| Tanh         | (-1, 1)      | Layer nascosti, alternativa pi√π centrata   |
| Softmax      | [0, 1], somma = 1 | Output per classificazione multiclasse |

---

## ‚ùì Domande Frequenti

**‚û§ Quale usare nei layer nascosti?**  
‚Üí ReLU (veloce e funziona bene) o Leaky ReLU (se ReLU blocca i neuroni negativi)

**‚û§ E nell‚Äôoutput per classificazione binaria (es. s√¨/no)?**  
‚Üí Sigmoid con **1 solo neurone**

**‚û§ E per classificazione a 3 o pi√π classi (es. tipo di fiore)?**  
‚Üí Softmax con **tanti neuroni quanti le classi**

**‚û§ Cos‚Äô√® il Vanishing Gradient?**  
‚Üí Quando la rete smette di imparare perch√© i gradienti diventano troppo piccoli (succede spesso con sigmoid o tanh)

---

## üß† Consiglio pratico:

- ‚úÖ **ReLU o Leaky ReLU** ‚Üí nei layer nascosti
- ‚úÖ **Sigmoid** ‚Üí output per 0/1
- ‚úÖ **Softmax** ‚Üí output per classificazione 3+ classi
