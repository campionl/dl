# Consigli per la creazione di una rete neurale

Quando creiamo una rete neurale √® importante riconoscere quando creare un nuovo livello e quando aggiungere un neurone ad un livello,
qui sotto c'√® una guida per queste decisioni.

## Guida: Numero di Neuroni per Layer in una Rete Neurale

| Caso / Configurazione                         | Esempio (neuroni per layer) | Motivo                                                                                       | Quando usarla                                                                                 |
|----------------------------------------------|------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| üîπ Input scarso, output ricco                 | 1 ‚Üí 4                        | Estrazione di molte feature da pochi input. Serve espansione di rappresentazione.            | Quando hai un input semplice ma vuoi modellare fenomeni complessi (es. 1 valore ‚Üí 4 categorie). |
| üîπ Input ricco, output semplice               | 8 ‚Üí 2                        | Compressione: i layer successivi imparano a sintetizzare le informazioni.                    | Es. immagini 28x28 ‚Üí classificazione binaria.                                                   |
| üîπ Struttura a clessidra (bottleneck)         | 16 ‚Üí 4 ‚Üí 16                  | Compressione e decompressione (tipo autoencoder). Il layer stretto forza l‚Äôapprendimento utile. | Compressione dati, denoising, feature extraction.                                               |
| üîπ Architettura piramidale (decrescente)      | 64 ‚Üí 32 ‚Üí 16                 | Riduzione progressiva della dimensionalit√† ‚Üí migliora generalizzazione.                      | Classificazione, regressione, riduzione del rumore.                                             |
| üîπ Architettura piramide inversa (crescente)  | 8 ‚Üí 16 ‚Üí 32                  | Usata per generazione o upscaling (es. GAN, decoder, generazione testi o immagini).          | Decoder, generatori (es. VAE decoder, GAN generator).                                           |
| üîπ Stesso numero di neuroni in ogni layer     | 16 ‚Üí 16 ‚Üí 16                 | Stessa capacit√† elaborativa a ogni livello. Rischia overfitting.                             | Quando non conosci bene i dati e vuoi una base uniforme.                                        |
| üîπ Troppi neuroni nel primo layer             | 128 ‚Üí 64 ‚Üí 32                | Rischia overfitting: troppa capacit√† sin dall'inizio.                                        | Da evitare se l‚Äôinput √® semplice o rumoroso.                                                    |
| üîπ Pochi neuroni nel primo layer              | 2 ‚Üí 8 ‚Üí 16                   | Primo layer perde informazione; il secondo tenta di ricostruirla ‚Üí inefficienza.             | Evitare se l‚Äôinput ha molte feature importanti.                                                 |
| üîπ Layer centrale troppo piccolo              | 64 ‚Üí 2 ‚Üí 64                  | Bottleneck troppo stretto ‚Üí perde troppe info.                                                | Utile solo se vuoi forzare un embedding molto compatto.                                         |
| üîπ Tanti layer con pochi neuroni              | 8 ‚Üí 8 ‚Üí 8 ‚Üí 8 ‚Üí 8            | Approfondisci la rete senza aumentare la capacit√† troppo.                                    | NLP, RNN, transformers con deep attention.                                                      |

---

## Considerazioni Importanti

- **Primo layer**: dovrebbe avere almeno tanti neuroni quante feature in input.
- **Ultimo layer**: tanti neuroni quanti valori in output (es. 1 per regressione, N per classificazione).
- **Layer nascosti**: variano in base a:
  - Complessit√† del problema
  - Quantit√† di dati disponibili
  - Obiettivo del modello (classificazione, compressione, generazione‚Ä¶)

---

## Tip Tecnici

- ‚úÖ Troppi neuroni/layers = **Overfitting**
- ‚ùå Troppo pochi = **Underfitting**
- üéØ Trova il giusto equilibrio con:
  - **Validazione cross**
  - **Early stopping**
  - **Regularizzazione**

## Quando Aggiungere Layer vs Neuroni in una Rete Neurale

| Cosa Modificare           | Quando Farlo                                                             | Effetto Principale                                                 | Esempio Applicativo                                               |
|---------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------|
| üîπ Aggiungere un **neurone** in un layer | Quando il modello **underfitta leggermente** e hai ancora margine di capacit√† | Aumenta la **capacit√† locale** del layer                            | Un layer da 16 neuroni ‚Üí 32 per migliorare accuratezza del training |
| üîπ Aggiungere **molti neuroni**         | Quando il layer √® **troppo piccolo** per catturare relazioni complesse          | Pi√π potenza computazionale ma rischio overfitting                   | Classificatore con molti input ma scarsi risultati                |
| üîπ Aggiungere **un nuovo layer**        | Quando la rete non riesce a catturare **relazioni gerarchiche o profonde**     | Aumenta la **profondit√†**, quindi astrazione progressiva             | Immagine: primo layer ‚Üí bordi, secondo ‚Üí forme, terzo ‚Üí oggetti   |
| üîπ Aggiungere **pi√π layer**             | Se il problema √® complesso e hai **tanti dati di addestramento**               | Profondit√† = pi√π astrazione, ma pi√π difficile da addestrare         | Riconoscimento facciale, NLP, visione profonda                    |
| üîπ Aggiungere layer **solo in coda**    | Quando vuoi aumentare la **decodifica finale o raffinamento**                  | Rafforza la parte decisionale, utile per classificazione             | Rete che migliora output finale (es. 10 classi)                   |
| üîπ Aggiungere layer **intermedi**       | Quando serve maggiore **trasformazione tra le feature**                        | Aumenta la trasformazione astratta tra input e output                | Modelli che imparano da dati multivariati complessi              |
| üîπ Ridurre neuroni                      | Per ridurre overfitting o aumentare generalizzazione                            | Meno parametri = meno rischio overfitting                           | Da 128 a 64 se accuracy test > accuracy train                     |
| üîπ Ridurre layer                        | Quando il modello √® troppo lento o non migliora con pi√π profondit√†              | Meno complessit√†, pi√π generalizzazione, pi√π veloce                   | Applicazioni embedded o mobile                                   |

---

## Regole Generali

| Situazione del Modello                  | Azione Consigliata                  |
|----------------------------------------|-------------------------------------|
| Overfitting (molta accuratezza train, bassa test) | ‚ûñ Neuroni o ‚ûñ layer / + regularizzazione |
| Underfitting (bassa accuratezza ovunque)         | ‚ûï Neuroni o ‚ûï layer                |
| Accuracy stabile ma bassa              | ‚ûï Layer per astrazione maggiore    |
| Accuracy altalenante                   | Controlla learning rate o batch size |
| Training troppo lento                  | ‚ûñ Layer o ‚ûñ Neuroni                 |

---

## Best Practice

- Aumenta **neuroni** quando vuoi migliorare una fase specifica.
- Aggiungi **layer** quando il problema richiede pi√π passi astratti o logici.
- Evita di farlo a caso: valuta con **validazione e curve di apprendimento**.

---

# üß† Tutti i Layer delle Reti Neurali - Spiegati in modo semplice

> Ogni layer √® come una fase della lavorazione di un'informazione. Vediamo cosa fanno.

---

# üìö Keras Layers ‚Äì Guida Completa

## üß± 1. Core Layers

| Layer                 | Descrizione                                                    | Uso                         |
|-----------------------|----------------------------------------------------------------|-----------------------------|
| `Dense(units)`        | Layer completamente connesso (fully connected).               | Classificatori, MLP         |
| `Activation('relu')`  | Applica una funzione di attivazione.                          | Attivazioni standalone      |
| `Dropout(rate)`       | Spegne neuroni casualmente durante il training.               | Regularizzazione            |
| `Flatten()`           | Appiattisce un tensore multidimensionale in 1D.               | Da CNN a Dense              |
| `Reshape(target_shape)`| Cambia la forma del tensore.                                 | Manipolazione della forma   |
| `Input(shape)`        | Definisce un tensore di input (solo in Functional API).       | Inizio rete funzionale      |

---

## üß† 2. Convolutional Layers

| Layer                  | Descrizione                                             | Uso                     |
|------------------------|---------------------------------------------------------|--------------------------|
| `Conv2D(filters, kernel_size)` | Applica convoluzioni su immagini 2D.        | Visione artificiale      |
| `Conv1D` / `Conv3D`     | Convoluzioni su dati 1D o 3D.                         | Audio, video             |
| `SeparableConv2D`       | Convoluzione pi√π leggera e veloce.                   | MobileNet, reti leggere  |
| `DepthwiseConv2D`       | Convoluzione per canale.                             | Architetture avanzate    |

---

## üåÄ 3. Pooling Layers

| Layer                      | Descrizione                                          | Uso                           |
|----------------------------|------------------------------------------------------|-------------------------------|
| `MaxPooling2D(pool_size)`  | Seleziona il valore massimo da ogni finestra.       | Riduce dimensioni spaziali    |
| `AveragePooling2D`         | Calcola la media in ogni regione.                   | Alternativa pi√π stabile       |
| `GlobalMaxPooling2D()`     | Ritorna il max di ogni feature map.                 | Classificatore compatto       |
| `GlobalAveragePooling2D()` | Media globale per ogni feature map.                 | Molto usato in MobileNet      |

---

## üìè 4. Normalizzazione e Rumore

| Layer                     | Descrizione                                            | Uso                        |
|---------------------------|--------------------------------------------------------|-----------------------------|
| `BatchNormalization()`    | Normalizza attivazioni batch per batch.               | Stabilizzazione e velocit√†  |
| `LayerNormalization()`    | Normalizza per ogni campione.                         | NLP, RNN                    |
| `GaussianNoise(stddev)`   | Aggiunge rumore casuale.                              | Regularizzazione            |
| `GaussianDropout(rate)`   | Versione rumorosa di Dropout.                         | Alternativa avanzata        |

---

## ‚è≥ 5. Reti Ricorrenti (RNN)

| Layer                 | Descrizione                                  | Uso                |
|-----------------------|----------------------------------------------|---------------------|
| `SimpleRNN(units)`    | RNN base.                                    | Sequenze brevi      |
| `LSTM(units)`         | Long Short-Term Memory.                      | Testi, serie temporali |
| `GRU(units)`          | Variante efficiente di LSTM.                 | NLP, serie temporali |

---

## üß© 6. Altri Layer Utili

| Layer                       | Descrizione                                  | Uso                     |
|-----------------------------|----------------------------------------------|--------------------------|
| `Embedding(input_dim, output_dim)` | Codifica parole in vettori.        | NLP, testi               |
| `RepeatVector(n)`           | Ripete un vettore n volte.                  | Encoder-Decoder          |
| `TimeDistributed(layer)`    | Applica un layer nel tempo (frame per frame).| Insieme a RNN            |
| `LeakyReLU(alpha)`          | Variante di ReLU con parte negativa.         | CNN avanzate             |
| `PReLU()`                   | ReLU con pendenza appresa.                   | Ottimizzazioni           |
| `ReLU(max_value, threshold, ...)` | Controllo dettagliato su ReLU.      | Versione configurabile   |

---

## üîÅ 7. Functional API

| Elemento                  | Descrizione                                 |
|---------------------------|---------------------------------------------|
| `Input(shape)`            | Crea l'input per una rete non-sequenziale.  |
| `Model(inputs, outputs)`  | Definisce un modello da input a output.     |

---

## ‚úÖ Consigli d'Uso

- üîπ Usa `Sequential()` per modelli semplici: `input_shape` va nel primo layer.
- üî∏ Usa `Input()` solo se lavori con la **Functional API**.
- ‚ö†Ô∏è In molti layer come `Conv2D` e `Dense`, puoi mettere direttamente l‚Äôattivazione con `activation='relu'`.

---



---

## üéØ In Sintesi

| Tipo di Layer       | Cosa Fa                                                |
|---------------------|--------------------------------------------------------|
| Dense               | Connessione completa tra neuroni                       |
| Conv2D              | Estrae pattern da immagini                             |
| MaxPooling2D        | Riduce dimensione prendendo il massimo                 |
| ReLU / Sigmoid / Tanh | Funzioni di attivazione                              |
| Softmax             | Converte in probabilit√†                                |
| Dropout             | Evita overfitting spegnendo neuroni a caso            |
| BatchNorm           | Stabilizza e accelera l‚Äôallenamento                    |
| Flatten             | Appiattisce per passare a layer densi                  |
| Embedding           | Codifica parole in numeri                              |
| Transformer         | "Attenzione" tra elementi in una sequenza              |
| LSTM / GRU          | Gestione memoria nel tempo per dati sequenziali        |

---

## ‚úÖ Vuoi un esempio pratico in codice?
Posso scriverti un esempio in PyTorch o Keras con questi layer, basta chiedere.
