# Consigli per la creazione di una rete neurale

Quando creiamo una rete neurale √® importante riconoscere quando creare un nuovo livello e quando aggiungere un neurone ad un livello,
qui sotto c'√® una guida per queste decisioni.

## Guida: Numero di Neuroni per Layer in una Rete Neurale

| Caso / Configurazione                         | Esempio (neuroni per layer) | Motivo                                                                                       | Quando usarla                                                                                 |
|----------------------------------------------|------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| üîπ Input scarso, output ricco                 | 1 ‚Üí 4                        | Estrazione di molte feature da pochi input. Serve espansione di rappresentazione.            | Quando hai un input semplice ma vuoi modellare fenomeni complessi (es. 1 valore ‚Üí 4 categorie). |
| üîπ Input ricco, output semplice               | 8 ‚Üí 2                        | Compressione: i layer successivi imparano a sintetizzare le informazioni.                    | Es. immagini 28x28 ‚Üí classificazione binaria.                                                   |
| üîπ Struttura a clessidra (bottleneck)         | 16 ‚Üí 4 ‚Üí 16                  | Compressione e decompressione (tipo autoencoder). Il layer stretto forza l‚Äôapprendimento utile. | Compressione dati, denoising, feature extraction.                                               |
| üîπ Architettura piramidale (decrescente)      | 64 ‚Üí 32 ‚Üí 16                 | Riduzione progressiva della dimensionalit√† ‚Üí migliora generalizzazione.                      | Classificazione, regressione, riduzione del rumore.                                             |
| üîπ Architettura piramide inversa (crescente)  | 8 ‚Üí 16 ‚Üí 32                  | Usata per generazione o upscaling (es. GAN, decoder, generazione testi o immagini).          | Decoder, generatori (es. VAE decoder, GAN generator).                                           |
| üîπ Stesso numero di neuroni in ogni layer     | 16 ‚Üí 16 ‚Üí 16                 | Stessa capacit√† elaborativa a ogni livello. Rischia overfitting.                             | Quando non conosci bene i dati e vuoi una base uniforme.                                        |
| üîπ Troppi neuroni nel primo layer             | 128 ‚Üí 64 ‚Üí 32                | Rischia overfitting: troppa capacit√† sin dall'inizio.                                        | Da evitare se l‚Äôinput √® semplice o rumoroso.                                                    |
| üîπ Pochi neuroni nel primo layer              | 2 ‚Üí 8 ‚Üí 16                   | Primo layer perde informazione; il secondo tenta di ricostruirla ‚Üí inefficienza.             | Evitare se l‚Äôinput ha molte feature importanti.                                                 |
| üîπ Layer centrale troppo piccolo              | 64 ‚Üí 2 ‚Üí 64                  | Bottleneck troppo stretto ‚Üí perde troppe info.                                                | Utile solo se vuoi forzare un embedding molto compatto.                                         |
| üîπ Tanti layer con pochi neuroni              | 8 ‚Üí 8 ‚Üí 8 ‚Üí 8 ‚Üí 8            | Approfondisci la rete senza aumentare la capacit√† troppo.                                    | NLP, RNN, transformers con deep attention.                                                      |

---

## Considerazioni Importanti

- **Primo layer**: dovrebbe avere almeno tanti neuroni quante feature in input.
- **Ultimo layer**: tanti neuroni quanti valori in output (es. 1 per regressione, N per classificazione).
- **Layer nascosti**: variano in base a:
  - Complessit√† del problema
  - Quantit√† di dati disponibili
  - Obiettivo del modello (classificazione, compressione, generazione‚Ä¶)

---

## Tip Tecnici

- ‚úÖ Troppi neuroni/layers = **Overfitting**
- ‚ùå Troppo pochi = **Underfitting**
- üéØ Trova il giusto equilibrio con:
  - **Validazione cross**
  - **Early stopping**
  - **Regularizzazione**

## Quando Aggiungere Layer vs Neuroni in una Rete Neurale

| Cosa Modificare           | Quando Farlo                                                             | Effetto Principale                                                 | Esempio Applicativo                                               |
|---------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------|
| üîπ Aggiungere un **neurone** in un layer | Quando il modello **underfitta leggermente** e hai ancora margine di capacit√† | Aumenta la **capacit√† locale** del layer                            | Un layer da 16 neuroni ‚Üí 32 per migliorare accuratezza del training |
| üîπ Aggiungere **molti neuroni**         | Quando il layer √® **troppo piccolo** per catturare relazioni complesse          | Pi√π potenza computazionale ma rischio overfitting                   | Classificatore con molti input ma scarsi risultati                |
| üîπ Aggiungere **un nuovo layer**        | Quando la rete non riesce a catturare **relazioni gerarchiche o profonde**     | Aumenta la **profondit√†**, quindi astrazione progressiva             | Immagine: primo layer ‚Üí bordi, secondo ‚Üí forme, terzo ‚Üí oggetti   |
| üîπ Aggiungere **pi√π layer**             | Se il problema √® complesso e hai **tanti dati di addestramento**               | Profondit√† = pi√π astrazione, ma pi√π difficile da addestrare         | Riconoscimento facciale, NLP, visione profonda                    |
| üîπ Aggiungere layer **solo in coda**    | Quando vuoi aumentare la **decodifica finale o raffinamento**                  | Rafforza la parte decisionale, utile per classificazione             | Rete che migliora output finale (es. 10 classi)                   |
| üîπ Aggiungere layer **intermedi**       | Quando serve maggiore **trasformazione tra le feature**                        | Aumenta la trasformazione astratta tra input e output                | Modelli che imparano da dati multivariati complessi              |
| üîπ Ridurre neuroni                      | Per ridurre overfitting o aumentare generalizzazione                            | Meno parametri = meno rischio overfitting                           | Da 128 a 64 se accuracy test > accuracy train                     |
| üîπ Ridurre layer                        | Quando il modello √® troppo lento o non migliora con pi√π profondit√†              | Meno complessit√†, pi√π generalizzazione, pi√π veloce                   | Applicazioni embedded o mobile                                   |

---

## Regole Generali

| Situazione del Modello                  | Azione Consigliata                  |
|----------------------------------------|-------------------------------------|
| Overfitting (molta accuratezza train, bassa test) | ‚ûñ Neuroni o ‚ûñ layer / + regularizzazione |
| Underfitting (bassa accuratezza ovunque)         | ‚ûï Neuroni o ‚ûï layer                |
| Accuracy stabile ma bassa              | ‚ûï Layer per astrazione maggiore    |
| Accuracy altalenante                   | Controlla learning rate o batch size |
| Training troppo lento                  | ‚ûñ Layer o ‚ûñ Neuroni                 |

---

## Best Practice

- Aumenta **neuroni** quando vuoi migliorare una fase specifica.
- Aggiungi **layer** quando il problema richiede pi√π passi astratti o logici.
- Evita di farlo a caso: valuta con **validazione e curve di apprendimento**.

